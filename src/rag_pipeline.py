"""
RAG Pipeline - –ø–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å FAISS –∏ Perplexity API
"""
import os
import sys
import json
from typing import Optional, Dict, Any, List, Literal
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
if __name__ == "__main__":
    sys.path.insert(0, str(Path(__file__).parent.parent))

import numpy as np

# –ò–º–ø–æ—Ä—Ç—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
try:
    from .config import DATA_DIR, TOP_K_DOCUMENTS
    from .models import pplx_chat
except ImportError:
    from src.config import DATA_DIR, TOP_K_DOCUMENTS
    from src.models import pplx_chat


# –¢–∏–ø—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π
Category = Literal[
    "–æ—Å–Ω–æ–≤—ã",
    "–ø–æ–¥–±–æ—Ä_–º–∞—Ç–µ—Ä–∏–∞–ª–∞",
    "–Ω–∞—Å—Ç—Ä–æ–π–∫–∞_–ø—Ä–∏–Ω—Ç–µ—Ä–∞",
    "–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞_–¥–µ—Ñ–µ–∫—Ç–æ–≤",
    "—Å–ª–∞–π—Å–µ—Ä",
    "–¥—Ä—É–≥–æ–µ"
]


class RAGPipeline:
    """
    –ü–æ–ª–Ω–∞—è RAG-—Å–∏—Å—Ç–µ–º–∞ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏:
    1. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä - –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞
    2. –ü–æ–∏—Å–∫–æ–≤–∏–∫ - –∏—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    3. –ö–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç - —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
    4. –í–∞–ª–∏–¥–∞—Ç–æ—Ä - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-—Å–∏—Å—Ç–µ–º—ã"""
        self.knowledge_base = []
        self.faiss_index = None
        self.embeddings_model = None
        self._load_knowledge_base()
        self._load_faiss_index()
    
    def _load_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        kb_path = DATA_DIR / "processed.jsonl"
        
        if not kb_path.exists():
            print(f"‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {kb_path}")
            return
        
        try:
            with open(kb_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        self.knowledge_base.append(json.loads(line))
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.knowledge_base)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
    
    def _load_faiss_index(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞"""
        faiss_path = DATA_DIR / "faiss_index"
        index_file = faiss_path / "index.faiss"
        
        if not index_file.exists():
            print(f"‚ö†Ô∏è FAISS –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {index_file}")
            print("–ó–∞–ø—É—Å—Ç–∏—Ç–µ: python -m src.embeddings_store_faiss")
            return
        
        try:
            import faiss
            self.faiss_index = faiss.read_index(str(index_file))
            print(f"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω ({self.faiss_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤)")
            
            from sentence_transformers import SentenceTransformer
            self.embeddings_model = SentenceTransformer('intfloat/multilingual-e5-large')
            print("‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        except ImportError:
            print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install faiss-cpu sentence-transformers")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ FAISS: {e}")
    
    def _classify_query(self, user_query: str) -> Category:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        system_prompt = (
            "–¢—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ 3D-–ø–µ—á–∞—Ç–∏. "
            "–û–ø—Ä–µ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞ –∏ –≤–µ—Ä–Ω–∏ –û–î–ù–û —Å–ª–æ–≤–æ –∏–∑ —Å–ø–∏—Å–∫–∞:\n"
            "- –æ—Å–Ω–æ–≤—ã\n"
            "- –ø–æ–¥–±–æ—Ä_–º–∞—Ç–µ—Ä–∏–∞–ª–∞\n"
            "- –Ω–∞—Å—Ç—Ä–æ–π–∫–∞_–ø—Ä–∏–Ω—Ç–µ—Ä–∞\n"
            "- –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞_–¥–µ—Ñ–µ–∫—Ç–æ–≤\n"
            "- —Å–ª–∞–π—Å–µ—Ä\n"
            "- –¥—Ä—É–≥–æ–µ\n\n"
            "–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º –∏–∑ —Å–ø–∏—Å–∫–∞."
        )
        
        try:
            response = pplx_chat(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_query}
                ],
                temperature=0.0,
                max_tokens=10
            )
            
            response_lower = response.lower().strip()
            for category in ["–æ—Å–Ω–æ–≤—ã", "–ø–æ–¥–±–æ—Ä_–º–∞—Ç–µ—Ä–∏–∞–ª–∞", "–Ω–∞—Å—Ç—Ä–æ–π–∫–∞_–ø—Ä–∏–Ω—Ç–µ—Ä–∞", 
                           "–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞_–¥–µ—Ñ–µ–∫—Ç–æ–≤", "—Å–ª–∞–π—Å–µ—Ä"]:
                if category in response_lower:
                    return category  # type: ignore
            
            return "–¥—Ä—É–≥–æ–µ"  # type: ignore
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            return "–¥—Ä—É–≥–æ–µ"  # type: ignore
    
    def _search_documents(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ FAISS"""
        if not self.faiss_index or not self.embeddings_model:
            return self._simple_text_search(query, top_k)
        
        try:
            query_embedding = self.embeddings_model.encode([query])[0]
            query_vector = np.array([query_embedding], dtype=np.float32)
            
            distances, indices = self.faiss_index.search(query_vector, top_k)
            
            results = []
            for idx in indices[0]:
                if 0 <= idx < len(self.knowledge_base):
                    doc = self.knowledge_base[idx].copy()
                    results.append(doc)
            
            return results
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ FAISS –ø–æ–∏—Å–∫–∞: {e}")
            return self._simple_text_search(query, top_k)
    
    def _simple_text_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫"""
        query_lower = query.lower()
        scored_docs = []
        
        for doc in self.knowledge_base:
            score = 0
            content = doc.get('content', '').lower()
            title = doc.get('title', '').lower()
            
            for word in query_lower.split():
                if len(word) > 2:
                    score += content.count(word)
                    score += title.count(word) * 3
            
            if score > 0:
                scored_docs.append((score, doc))
        
        scored_docs.sort(reverse=True, key=lambda x: x[0])
        return [doc for score, doc in scored_docs[:top_k]]
    
    def _generate_answer(
        self, 
        user_query: str, 
        category: Category,
        documents: List[Dict[str, Any]],
        dialog_context: str = ""
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        if not documents:
            return self._generate_fallback_answer(user_query, category)
        
        context_parts = []
        for i, doc in enumerate(documents, 1):
            title = doc.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
            content = doc.get('content', '')[:800]
            url = doc.get('source_url', 'N/A')
            
            context_parts.append(
                f"[–î–æ–∫—É–º–µ–Ω—Ç {i}]\n"
                f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n"
                f"–ò—Å—Ç–æ—á–Ω–∏–∫: {url}\n"
                f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {content}\n"
            )
        
        context_text = "\n".join(context_parts)
        
        system_prompt = (
            "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ 3D-–ø–µ—á–∞—Ç–∏ —Å –º–Ω–æ–≥–æ–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º. "
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –¥–∞—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–µ–∑–Ω—ã–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç.\n\n"
            "–í–ê–ñ–ù–û:\n"
            "- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç—ã –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n"
            "- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º\n"
            "- –î–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã, —Å–∫–æ—Ä–æ—Å—Ç–∏, –∏ —Ç.–¥.)\n"
            "- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç –ø–æ –ø—É–Ω–∫—Ç–∞–º"
        )
        
        user_prompt = (
            f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}\n"
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {dialog_context or '–ù–µ—Ç'}\n\n"
            f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π:\n{context_text}\n\n"
            f"–í–æ–ø—Ä–æ—Å: {user_query}\n\n"
            "–î–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç."
        )
        
        try:
            answer = pplx_chat(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.4,
                max_tokens=1200
            )
            
            answer += "\n\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n"
            for i, doc in enumerate(documents, 1):
                title = doc.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
                url = doc.get('source_url', 'N/A')
                answer += f"{i}. {title} - {url}\n"
            
            return answer
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return self._generate_fallback_answer(user_query, category)
    
    def _generate_fallback_answer(self, query: str, category: Category) -> str:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –æ—Ç–≤–µ—Ç"""
        return (
            f"–ü–æ –∑–∞–ø—Ä–æ—Å—É '{query}' (–∫–∞—Ç–µ–≥–æ—Ä–∏—è: {category}):\n\n"
            "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç. "
            "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ API."
        )
    
    def _validate_safety(self, answer: str) -> str:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        try:
            system_prompt = (
                "–ü—Ä–æ–≤–µ—Ä—å –æ—Ç–≤–µ—Ç –Ω–∞ –æ–ø–∞—Å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ 3D-–ø–µ—á–∞—Ç–∏. "
                "–ï—Å–ª–∏ –≤—Å—ë –≤ –ø–æ—Ä—è–¥–∫–µ - –≤–µ—Ä–Ω–∏ –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô."
            )
            
            validated = pplx_chat(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"–ü—Ä–æ–≤–µ—Ä—å:\n\n{answer}"}
                ],
                temperature=0.1,
                max_tokens=1500
            )
            return validated
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            return "‚ö†Ô∏è –°–æ–±–ª—é–¥–∞–π—Ç–µ —Ç–µ—Ö–Ω–∏–∫—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.\n\n" + answer
    
    def query(
        self, 
        question: str, 
        top_k: int = 3,
        dialog_context: str = "",
        enable_validation: bool = True
    ) -> str:
        """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        if not self.knowledge_base:
            return "‚ùå –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞."
        
        try:
            category = self._classify_query(question)
            documents = self._search_documents(question, top_k)
            answer = self._generate_answer(question, category, documents, dialog_context)
            
            if enable_validation:
                answer = self._validate_safety(answer)
            
            return answer
            
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞: {str(e)}"


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RAG Pipeline\n")
    
    rag = RAGPipeline()
    
    if rag.knowledge_base:
        print(f"üìö –ë–∞–∑–∞: {len(rag.knowledge_base)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n")
        
        test_query = "–ø—Ä–æ–±–ª–µ–º—ã —Å –ø–µ—á–∞—Ç—å—é ABS"
        print(f"‚ùì –ó–∞–ø—Ä–æ—Å: {test_query}\n")
        
        response = rag.query(test_query, top_k=3, enable_validation=False)
        print("ü§ñ –û—Ç–≤–µ—Ç:")
        print(response)
    else:
        print("‚ùå –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞")
