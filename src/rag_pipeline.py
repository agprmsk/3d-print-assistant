"""
RAG Pipeline - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å FAISS –∏ Perplexity API
"""
import os
import sys
import json
import time
from typing import Optional, Dict, Any, List, Literal
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
if __name__ == "__main__":
    sys.path.insert(0, str(Path(__file__).parent.parent))

import numpy as np

# –ò–º–ø–æ—Ä—Ç—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
try:
    from .config import DATA_DIR, TOP_K_DOCUMENTS
    from .models import pplx_chat
except ImportError:
    from src.config import DATA_DIR, TOP_K_DOCUMENTS
    from src.models import pplx_chat

# –¢–∏–ø—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π
Category = Literal[
    "–æ—Å–Ω–æ–≤—ã",
    "–ø–æ–¥–±–æ—Ä_–º–∞—Ç–µ—Ä–∏–∞–ª–∞",
    "–Ω–∞—Å—Ç—Ä–æ–π–∫–∞_–ø—Ä–∏–Ω—Ç–µ—Ä–∞",
    "–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞_–¥–µ—Ñ–µ–∫—Ç–æ–≤",
    "—Å–ª–∞–π—Å–µ—Ä",
    "–¥—Ä—É–≥–æ–µ"
]


class RAGPipeline:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è RAG-—Å–∏—Å—Ç–µ–º–∞ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏:
    1. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä - –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞ (–ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º)
    2. –ü–æ–∏—Å–∫–æ–≤–∏–∫ - –∏—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (FAISS)
    3. –ö–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç - —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç (Perplexity)
    4. –í–∞–ª–∏–¥–∞—Ç–æ—Ä - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å (–ø—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–ª–æ–≤)
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-—Å–∏—Å—Ç–µ–º—ã"""
        self.knowledge_base = []
        self.faiss_index = None
        self.embeddings_model = None
        self._load_knowledge_base()
        self._load_faiss_index()
    
    def _load_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        kb_path = DATA_DIR / "processed.jsonl"
        
        if not kb_path.exists():
            print(f"‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {kb_path}")
            return
        
        try:
            with open(kb_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        self.knowledge_base.append(json.loads(line))
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.knowledge_base)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
    
    def _load_faiss_index(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞"""
        faiss_path = DATA_DIR / "faiss_index"
        index_file = faiss_path / "index.faiss"
        
        if not index_file.exists():
            print(f"‚ö†Ô∏è FAISS –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {index_file}")
            print("–ó–∞–ø—É—Å—Ç–∏—Ç–µ: python -m src.embeddings_store_faiss")
            return
        
        try:
            import faiss
            self.faiss_index = faiss.read_index(str(index_file))
            print(f"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω ({self.faiss_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤)")
            
            from sentence_transformers import SentenceTransformer
            self.embeddings_model = SentenceTransformer('intfloat/multilingual-e5-large')
            print("‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        except ImportError:
            print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install faiss-cpu sentence-transformers")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ FAISS: {e}")
    
    def _classify_query(self, user_query: str) -> Category:
        """–ë—ã—Å—Ç—Ä–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–ë–ï–ó LLM)"""
        query_lower = user_query.lower()
        
        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤
        if any(word in query_lower for word in ["–∑–∞–±–∏–ª–æ—Å—å", "–∑–∞–±–∏–ª–∞—Å—å", "—Å–æ–ø–ª–æ", "—ç–∫—Å—Ç—Ä—É–¥–µ—Ä", "–¥–µ—Ñ–µ–∫—Ç", "—Å–ª–æ–∏", "–ø–æ–ª–æ—Å—ã", "—Ç—Ä–µ—â–∏–Ω—ã", "—Ä–∞—Å—Å–ª–∞–∏–≤–∞–µ—Ç—Å—è", "–Ω–µ –ø—Ä–∏–ª–∏–ø–∞–µ—Ç", "–æ—Ç–∫–ª–µ–∏–≤–∞–µ—Ç—Å—è"]):
            return "–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞_–¥–µ—Ñ–µ–∫—Ç–æ–≤"
        
        # –ü–æ–¥–±–æ—Ä –º–∞—Ç–µ—Ä–∏–∞–ª–∞
        elif any(word in query_lower for word in ["–º–∞—Ç–µ—Ä–∏–∞–ª", "pla", "abs", "petg", "filament", "–ø–ª–∞—Å—Ç–∏–∫", "—Ñ–∏–ª–∞–º–µ–Ω—Ç", "tpu", "nylon"]):
            return "–ø–æ–¥–±–æ—Ä_–º–∞—Ç–µ—Ä–∏–∞–ª–∞"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–∏–Ω—Ç–µ—Ä–∞
        elif any(word in query_lower for word in ["–Ω–∞—Å—Ç—Ä–æ–π–∫–∞", "–∫–∞–ª–∏–±—Ä–æ–≤–∫–∞", "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞", "—Å–∫–æ—Ä–æ—Å—Ç—å", "—Ä–µ—Ç—Ä–∞–∫—Ç", "–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å", "–æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞—Ç—å"]):
            return "–Ω–∞—Å—Ç—Ä–æ–π–∫–∞_–ø—Ä–∏–Ω—Ç–µ—Ä–∞"
        
        # –°–ª–∞–π—Å–µ—Ä
        elif any(word in query_lower for word in ["—Å–ª–∞–π—Å–µ—Ä", "cura", "prusaslicer", "slicer", "–Ω–∞—Ä–µ–∑–∫–∞"]):
            return "—Å–ª–∞–π—Å–µ—Ä"
        
        # –û—Å–Ω–æ–≤—ã / –≤—ã–±–æ—Ä –ø—Ä–∏–Ω—Ç–µ—Ä–∞
        elif any(word in query_lower for word in ["–Ω–∞—á–∏–Ω–∞—é", "–Ω–æ–≤–∏—á–æ–∫", "–ø–µ—Ä–≤—ã–π", "–æ—Å–Ω–æ–≤—ã", "–≤—ã–±—Ä–∞—Ç—å", "–ø—Ä–∏–Ω—Ç–µ—Ä", "–∫–∞–∫–æ–π –ø—Ä–∏–Ω—Ç–µ—Ä"]):
            return "–æ—Å–Ω–æ–≤—ã"
        
        # –û—Å—Ç–∞–ª—å–Ω–æ–µ
        else:
            return "–¥—Ä—É–≥–æ–µ"
    
    def _search_documents(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ FAISS"""
        if not self.faiss_index or not self.embeddings_model:
            return self._simple_text_search(query, top_k)
        
        try:
            query_embedding = self.embeddings_model.encode([query])[0]
            query_vector = np.array([query_embedding], dtype=np.float32)
            
            distances, indices = self.faiss_index.search(query_vector, top_k)
            
            results = []
            for idx in indices[0]:
                if 0 <= idx < len(self.knowledge_base):
                    doc = self.knowledge_base[idx].copy()
                    results.append(doc)
            
            return results
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ FAISS –ø–æ–∏—Å–∫–∞: {e}")
            return self._simple_text_search(query, top_k)
    
    def _simple_text_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫"""
        query_lower = query.lower()
        scored_docs = []
        
        for doc in self.knowledge_base:
            score = 0
            content = doc.get('content', '').lower()
            title = doc.get('title', '').lower()
            
            for word in query_lower.split():
                if len(word) > 2:
                    score += content.count(word)
                    score += title.count(word) * 3
            
            if score > 0:
                scored_docs.append((score, doc))
        
        scored_docs.sort(reverse=True, key=lambda x: x[0])
        return [doc for score, doc in scored_docs[:top_k]]
    
    def _generate_answer(
        self, 
        user_query: str, 
        category: Category,
        documents: List[Dict[str, Any]],
        dialog_context: str = ""
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Perplexity"""
        if not documents:
            return self._generate_fallback_answer(user_query, category)
        
        context_parts = []
        for i, doc in enumerate(documents, 1):
            title = doc.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
            content = doc.get('content', '')[:800]
            url = doc.get('source_url', 'N/A')
            
            context_parts.append(
                f"[–î–æ–∫—É–º–µ–Ω—Ç {i}]\n"
                f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n"
                f"–ò—Å—Ç–æ—á–Ω–∏–∫: {url}\n"
                f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {content}\n"
            )
        
        context_text = "\n".join(context_parts)
        
        system_prompt = (
            "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ 3D-–ø–µ—á–∞—Ç–∏ —Å –º–Ω–æ–≥–æ–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º. "
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –¥–∞—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–µ–∑–Ω—ã–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç.\n\n"
            "–í–ê–ñ–ù–û:\n"
            "- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç—ã –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n"
            "- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º\n"
            "- –î–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã, —Å–∫–æ—Ä–æ—Å—Ç–∏, –∏ —Ç.–¥.)\n"
            "- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç –ø–æ –ø—É–Ω–∫—Ç–∞–º"
        )
        
        user_prompt = (
            f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}\n"
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {dialog_context or '–ù–µ—Ç'}\n\n"
            f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π:\n{context_text}\n\n"
            f"–í–æ–ø—Ä–æ—Å: {user_query}\n\n"
            "–î–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç."
        )
        
        try:
            answer = pplx_chat(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.4,
                max_tokens=1200
            )
            
            answer += "\n\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n"
            for i, doc in enumerate(documents, 1):
                title = doc.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
                url = doc.get('source_url', 'N/A')
                answer += f"{i}. {title} - {url}\n"
            
            return answer
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return self._generate_fallback_answer(user_query, category)
    
    def _generate_fallback_answer(self, query: str, category: Category) -> str:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –æ—Ç–≤–µ—Ç"""
        return (
            f"–ü–æ –∑–∞–ø—Ä–æ—Å—É '{query}' (–∫–∞—Ç–µ–≥–æ—Ä–∏—è: {category}):\n\n"
            "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç. "
            "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ API –∏–ª–∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
        )
    
    def _validate_safety(self, answer: str) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–ø–∞—Å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–ë–ï–ó LLM)"""
        dangerous_keywords = [
            "—Ç–æ–∫—Å–∏—á–Ω", "—è–¥–æ–≤–∏—Ç", "–≤–∑—Ä—ã–≤–æ–æ–ø–∞—Å–Ω", "–≤–∑—Ä—ã–≤", 
            "–≥–æ—Ä—é—á", "–ª–µ–≥–∫–æ–≤–æ—Å–ø–ª–∞–º–µ–Ω—è", "–ø–æ–∂–∞—Ä", "–æ—Ç—Ä–∞–≤–ª–µ–Ω"
        ]
        
        answer_lower = answer.lower()
        has_danger = any(keyword in answer_lower for keyword in dangerous_keywords)
        
        if has_danger:
            return "‚ö†Ô∏è –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨: –°–æ–±–ª—é–¥–∞–π—Ç–µ —Ç–µ—Ö–Ω–∏–∫—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏.\n\n" + answer
        
        return answer
    
    def query(
        self, 
        question: str, 
        top_k: int = 3,
        dialog_context: str = "",
        enable_validation: bool = True
    ) -> str:
        """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏"""
        if not self.knowledge_base:
            return "‚ùå –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞."
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–µ–º—É 3D-–ø–µ—á–∞—Ç–∏
        relevant_keywords = [
            "–ø—Ä–∏–Ω—Ç–µ—Ä", "–ø–µ—á–∞—Ç", "3d", "pla", "abs", "petg", "—Å–æ–ø–ª–æ", 
            "—ç–∫—Å—Ç—Ä—É–¥–µ—Ä", "—Å–ª–∞–π—Å–µ—Ä", "—Ñ–∏–ª–∞–º–µ–Ω—Ç", "–º–æ–¥–µ–ª—å", "—Å–ª–æ–π"
        ]
        if not any(kw in question.lower() for kw in relevant_keywords):
            return (
                "–Ø —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Å—å –Ω–∞ –≤–æ–ø—Ä–æ—Å–∞—Ö –æ 3D-–ø–µ—á–∞—Ç–∏. "
                "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ –≤—ã–±–æ—Ä–µ –ø—Ä–∏–Ω—Ç–µ—Ä–∞, "
                "–Ω–∞—Å—Ç—Ä–æ–π–∫–µ –ø–µ—á–∞—Ç–∏, —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–∏ –¥–µ—Ñ–µ–∫—Ç–æ–≤)."
            )
        
        try:
            start = time.time()
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–ë–ï–ó LLM - –º–≥–Ω–æ–≤–µ–Ω–Ω–æ)
            t1 = time.time()
            category = self._classify_query(question)
            print(f"‚è±Ô∏è –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {time.time() - t1:.2f}s (–∫–∞—Ç–µ–≥–æ—Ä–∏—è: {category})")
            
            # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (FAISS - –±—ã—Å—Ç—Ä–æ)
            t2 = time.time()
            documents = self._search_documents(question, top_k)
            print(f"‚è±Ô∏è –ü–æ–∏—Å–∫: {time.time() - t2:.2f}s (–Ω–∞–π–¥–µ–Ω–æ: {len(documents)} –¥–æ–∫.)")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (Perplexity - –æ—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞)
            t3 = time.time()
            answer = self._generate_answer(question, category, documents, dialog_context)
            print(f"‚è±Ô∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: {time.time() - t3:.2f}s")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–ë–ï–ó LLM - –º–≥–Ω–æ–≤–µ–Ω–Ω–æ)
            if enable_validation:
                t4 = time.time()
                answer = self._validate_safety(answer)
                print(f"‚è±Ô∏è –í–∞–ª–∏–¥–∞—Ü–∏—è: {time.time() - t4:.2f}s")
            
            print(f"‚è±Ô∏è –ò–¢–û–ì–û: {time.time() - start:.2f}s")
            return answer
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞: {str(e)}"


# –¢–µ—Å—Ç
