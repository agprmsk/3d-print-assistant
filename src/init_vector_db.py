# src/init_vector_db.py
import json
import os
from embeddings_store_faiss import EmbeddingsStoreFAISS

def load_processed_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    data_files = [
        "data/processed_data.json",
        "data/test_dataset.json",
        "data/raw/3dtoday_articles.json"
    ]
    
    for data_path in data_files:
        if os.path.exists(data_path):
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {data_path}...")
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data, data_path
    
    print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏:")
    for path in data_files:
        print(f"   - {path}")
    return None, None

def init_vector_db():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    data, source_path = load_processed_data()
    
    if not data:
        return
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π
    if isinstance(data, dict):
        articles = data.get('articles', [])
    elif isinstance(data, list):
        articles = data
    else:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö: {type(data)}")
        return
    
    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –∏–∑ {source_path}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    print("\nüì¶ –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
    store = EmbeddingsStoreFAISS()
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    for article in articles:
        content = article.get('content') or article.get('text') or article.get('body')
        if content:
            metadata = {
                'title': article.get('title', ''),
                'url': article.get('url', ''),
                'category': article.get('category', '')
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–ø—Ä—è–º—É—é –≤ —Å–ø–∏—Å–∫–∏
            store.documents.append(content)
            store.metadatas.append(metadata)
    
    if not store.documents:
        print("‚ùå –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º")
        print(f"–ü—Ä–∏–º–µ—Ä —ç–ª–µ–º–µ–Ω—Ç–∞: {articles[0] if articles else '–ù–µ—Ç —ç–ª–µ–º–µ–Ω—Ç–æ–≤'}")
        return
    
    print(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω–æ {len(store.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –∏–Ω–¥–µ–∫—Å–∞
    print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –∏–Ω–¥–µ–∫—Å–∞...")
    embeddings = store.model.encode(
        store.documents,
        show_progress_bar=True,
        batch_size=32
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞
    import faiss
    import numpy as np
    
    embeddings_np = np.array(embeddings).astype('float32')
    dimension = embeddings_np.shape[1]
    
    store.index = faiss.IndexFlatL2(dimension)
    store.index.add(embeddings_np)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
    store.save()
    
    print(f"\n‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!")
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   - –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(store.documents)}")
    print(f"   - –†–∞–∑–º–µ—Ä –∏–Ω–¥–µ–∫—Å–∞: {store.index.ntotal}")
    print(f"   - –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {dimension}")

if __name__ == "__main__":
    init_vector_db()
