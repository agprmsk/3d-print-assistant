# src/scraper_3dtoday.py
import requests
from bs4 import BeautifulSoup
import time
import json
import os
from typing import List, Dict

class WikiScraper3DToday:
    """–°–∫—Ä–∞–ø–µ—Ä –¥–ª—è —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–µ–π —Å 3DToday Wiki"""
    
    def __init__(self, base_url: str = "https://3dtoday.ru"):
        self.base_url = base_url
        self.wiki_url = f"{base_url}/wiki"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.articles = []
    
    def get_article_links(self, max_articles: int = 50) -> List[str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏ –∏–∑ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã Wiki
        
        Args:
            max_articles: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–±–æ—Ä–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ URL —Å—Ç–∞—Ç–µ–π
        """
        print(f"üîç –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏ –∏–∑ {self.wiki_url}...")
        
        try:
            response = requests.get(self.wiki_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏
            article_links = []
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            selectors = [
                'a[href*="/wiki/"]',  # –í—Å–µ —Å—Å—ã–ª–∫–∏ —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ /wiki/
                '.wiki-list a',
                '.article-list a',
                '.content a[href^="/wiki/"]'
            ]
            
            for selector in selectors:
                links = soup.select(selector)
                for link in links:
                    href = link.get('href', '')
                    if href and href.startswith('/wiki/') and href != '/wiki' and href != '/wiki/':
                        full_url = f"{self.base_url}{href}"
                        if full_url not in article_links:
                            article_links.append(full_url)
                    
                    if len(article_links) >= max_articles:
                        break
                
                if len(article_links) >= max_articles:
                    break
            
            print(f"‚úì –ù–∞–π–¥–µ–Ω–æ {len(article_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏")
            return article_links[:max_articles]
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫: {e}")
            return []
    
    def scrape_article(self, url: str) -> Dict:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç—å–∏
        
        Args:
            url: URL —Å—Ç–∞—Ç—å–∏
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Å—Ç–∞—Ç—å–∏
        """
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_elem = soup.find('h1')
            title = title_elem.text.strip() if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_selectors = [
                '.wiki-content',
                '.article-content',
                '.content',
                'article',
                '.main-content'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                    for tag in content_elem(['script', 'style', 'nav', 'footer']):
                        tag.decompose()
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç
                    content = content_elem.get_text(separator='\n', strip=True)
                    if len(content) > 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        break
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ URL
            category = "–û–±—â–µ–µ"
            url_parts = url.split('/')
            if len(url_parts) > 4:
                category = url_parts[4].replace('-', ' ').title()
            
            return {
                'title': title,
                'url': url,
                'category': category,
                'content': content,
                'content_length': len(content)
            }
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {e}")
            return None
    
    def scrape_articles(self, max_articles: int = 50, delay: float = 2.0):
        """
        –°–±–æ—Ä —Å—Ç–∞—Ç–µ–π —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
        Args:
            max_articles: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π
            delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
        """
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫
        article_urls = self.get_article_links(max_articles)
        
        if not article_urls:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏")
            return
        
        print(f"\nüì• –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä {len(article_urls)} —Å—Ç–∞—Ç–µ–π...")
        
        for i, url in enumerate(article_urls, 1):
            print(f"\n[{i}/{len(article_urls)}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {url}")
            
            article = self.scrape_article(url)
            
            if article and article['content_length'] > 100:
                self.articles.append(article)
                print(f"   ‚úì –°–æ–±—Ä–∞–Ω–æ {article['content_length']} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                print(f"   ‚ö† –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)")
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < len(article_urls):
                time.sleep(delay)
        
        print(f"\n‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(self.articles)} —Å—Ç–∞—Ç–µ–π")
    
    def save_articles(self, output_file: str = "data/raw/3dtoday_articles.json"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –≤ JSON"""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.articles, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   - –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(self.articles)}")
        
        if self.articles:
            total_chars = sum(a['content_length'] for a in self.articles)
            avg_chars = total_chars // len(self.articles)
            print(f"   - –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {total_chars:,}")
            print(f"   - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_chars:,} —Å–∏–º–≤–æ–ª–æ–≤")
    
    def run(self, max_articles: int = 50):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("üöÄ –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∞–ø–µ—Ä–∞ 3DToday Wiki\n")
        
        self.scrape_articles(max_articles=max_articles)
        self.save_articles()

if __name__ == "__main__":
    scraper = WikiScraper3DToday()
    # –°–æ–±–∏—Ä–∞–µ–º 90 —Å—Ç–∞—Ç–µ–π —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π 2 —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
    scraper.run(max_articles=90)
